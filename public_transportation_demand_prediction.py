# -*- coding: utf-8 -*-
"""Public Transportation Demand Prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KIEw00h3dDhHiH-WVVqngfnxEnjOudLM
"""

# Import Libraries
import pandas as pd

# Load the CSV file
df = pd.read_csv("/content/datasets/train_revised.csv")

# Display the first 10 rows of the DataFrame
df[:10]

# check the number of rows and columns in the DataFrame
print(df.shape)

# Check for columns with missing values
null_columns = df.columns[df.isnull().any()]

# Print the names of the columns with missing values
print(null_columns)

# Check for columns with no missing values
non_null_columns = df.columns[df.notnull().all()]

# Print the names of the columns with no missing values
print(non_null_columns)

# Covert the 'travel_time' column to datetime
df['travel_date'] = pd.to_datetime(df['travel_date'])

# Create a mapping from integers to day names
day_names = {
    0: 'Monday',
    1: 'Tuesday',
    2: 'Wednesday',
    3: 'Thursday',
    4: 'Friday',
    5: 'Saturday',
    6: 'Sunday'

}

# Convert the 'travel_date' column to the corresponding day of the week
df['travel_date'] = df['travel_date'].dt.weekday.map(day_names)

# Print the first 10 rows of the DataFrame
df[:10]

# Group the data by route and day of the week
grouped_data = df.groupby(['travel_from', 'travel_to', 'travel_date'])

# Compute the average number of tickets purchased for each route on Sundays
route_averages = grouped_data['max_capacity'].mean()

# Filter the data to only include routes on Sundays
sunday_routes = route_averages[route_averages.index.get_level_values('travel_date') == 'Sunday']

# Sort the routes by the average number of max_capacity purchased in decreasing order
sunday_routes.sort_values(ascending=False, inplace=True)

# Print the top 7 most traveled routes for a Sunday
print(sunday_routes.head(7))

# Convert the 'travel_time' column to datetime
df['travel_time'] = pd.to_datetime(df['travel_time'], format='%H:%M:%S')

# Print the first 10 rows of the DataFrame
df[:10]

# Convert the 'travel_time' column to datetime
df['travel_time'] = pd.to_datetime(df['travel_time'], errors='coerce')

# Check the number of rows in the 'df' DataFrame where 'travel_from' has the value 'Kijauri'
print(df[df['travel_from'] == 'Kijauri'].shape[0])

# Check the number of rows in the 'df' DataFrame where 'travel_time' is before 07:30
print(df[df['travel_time'] < '07:30:00'].shape[0])

from datetime import datetime

# Group the data by 'travel_from' and 'travel_time'
grouped_data = df.groupby(['travel_from', 'travel_time'])

# Compute the probability of taking a Shuttle for each combination of 'travel_from' and 'travel_time'
probabilities = grouped_data['travel_from'].apply(lambda x: (x == 'Shuttle').mean())

# Filter the data to only include departures before 07:30 from Kijauri
kijauri_shuttle = probabilities[(probabilities.index.get_level_values('travel_time') < '07:30') & (probabilities.index.get_level_values('travel_from') == 'Kijauri')]

# Print the probability of taking a Shuttle for departures before 07:30 from Kijauri
print(kijauri_shuttle)

from collections import Counter

# Extract payment references that contain 'MK'
references = df[df['payment_receipt'].str.contains('MK')]['payment_receipt']

# Initialize an empty list to store the characters that come after 'MK'
characters = []

# Iterate over the payment references
for reference in references:

  # Find the index of the first occurence of 'MK' in the reference
  index = reference.index('MK')

  # Extract the character that comes after 'MK' in the reference
  if index + 2 < len(reference):
    character = reference[index+2]

  # Add the character to the list
  characters.append(character)

# Count the frequency of each character in the list
counter = Counter(characters)

# Find the character that appears most frequently
most_common = counter.most_common(1)

# Print the most common character
print(most_common)